


<!DOCTYPE HTML>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
	<head>
		<meta charset="utf-8">
		<meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

		<meta http-equiv="content-type" content="text/html; charset=UTF-8"/>
		<link rel="stylesheet" href="https://static.cloud.coveo.com/searchui/v2.4382/css/CoveoFullSearch.css"/>
		<meta http-equiv="X-UA-Compatible" content="IE=edge"/>
		<meta name="viewport" content="width=device-width, initial-scale=1.0"/>
		<meta name="description"/>
		<meta name="keywords"/>
		<meta property="og:title" content=""/>
		<meta property="og:description"/>
		<!-- favicon -->
		<link rel="icon" type="image/vnd.microsoft.icon" href="../../../_static/favicon.ico"/>
		<link rel="shortcut icon" type="image/vnd.microsoft.icon" href="../../../_static/favicon.ico"/>
		<!-- Fonts -->
		<link href="https://fonts.googleapis.com/css?family=Roboto:300,400,500" rel="stylesheet" type="text/css"/>

	
		<!-- Google Tag Manager -->
	<script type="text/plain" class="optanon-category-C0002">(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':
	new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],
	j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src=
	'//www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);
	})(window,document,'script','dataLayer','GTM-5RHQV7');</script>
	<!-- End Google Tag Manager -->
	
        <!-- Google Tag Manager -->
        <noscript><iframe src="//www.googletagmanager.com/ns.html?id=GTM-5RHQV7" height="0" width="0" style="display:none;visibility:hidden" class="optanon-category-C0002"></iframe></noscript>
        <!-- End Google Tag Manager -->	

  
  
  
  

  
      <script type="text/javascript" src="../../../_static/js/jquery.min.js"></script>
	  <script type="text/javascript" src="../../../_static/js/gtm.js"></script>
  <script type="text/javascript" src="../../../_static/js/modernizr.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/d3dd8c60ed.js"></script>
    <script type="text/javascript" src="../../../_static/js/common-ui-all.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/jquery-ui.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/CoveoJsSearch.Lazy.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/linkid.js"></script>
    <script type="text/javascript" src="../../../_static/js/Searchbox.min.js"></script>
    <script type="text/javascript" src="../../../_static/js/header-footer.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../../" src="../../../_static/documentation_options.js"></script>
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../_static/doctools.js"></script>
    
    <script type="text/javascript" src="../../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/common-ui-all.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/header-footer.min.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/pro.min.css" media="all" />
  <link rel="stylesheet" href="../../../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../../../_static/css/theme.css" type="text/css" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="VVAS GStreamer Plug-ins for Embedded Platforms" href="../embedded-plugins.html" />
    <link rel="prev" title="Smart Model Select Application" href="../smart_model_select.html" /> 
	</head>
	<body>
		<div class="xilinx-bs3"/>
		<div class="root responsivegrid">
			<div class="aem-Grid aem-Grid--16 aem-Grid--default--16 aem-Grid--large--16 aem-Grid--xlarge--16 aem-Grid--xxlarge--16 aem-Grid--xxxlarge--16 ">
 <div class="xilinxExperienceFragments experiencefragment aem-GridColumn aem-GridColumn--default--12">

    
    

    



<div class="xf-content-height">
    

<div class="aem-Grid aem-Grid--16 aem-Grid--default--16 ">
    
    <div class="header parbase aem-GridColumn aem-GridColumn--default--12"><noindex>
	<header data-component="header" class="site-header">
		<nav class="navbar navbar-default">
			<div class="container-fluid main-nav">
				<div class="row">
					<div class="navbar-column col-xs-4 col-sm-5">
						<ul class="nav navbar-nav hidden-xs">
							<li>
									<button onclick="window.location.href=&#39;https://www.xilinx.com/applications.html&#39;;">Solutions</button>
								</li>
							<li>
									<button onclick="window.location.href=&#39;https://www.xilinx.com/products/silicon-devices.html&#39;;">Products</button>
								</li>
							<li>
									<button onclick="window.location.href=&#39;https://www.xilinx.com/about/company-overview.html&#39;;">Company</button>
								</li>
							</ul>
							
						<ul class="nav navbar-nav hidden-sm hidden-md hidden-lg">
							<li>
									<button data-target="#header-container-0" data-function="toggle">Solutions</button>
								</li>
							<li>
									<button data-target="#header-container-1" data-function="toggle">Products</button>
								</li>
							<li>
									<button data-target="#header-container-2" data-function="toggle">Company</button>
								</li>
							</ul>
							
						<div id="header-container-0" class="menu-container">
								<div class="navbar-nav-container">
									<ul class="nav navbar-nav">
										<li>
												<button onclick="window.location.href=&#39;https://www.xilinx.com/applications.html&#39;;">Solutions</button>
												</li>
										<li>
												<button onclick="window.location.href=&#39;https://www.xilinx.com/products/silicon-devices.html&#39;;">Products</button>
												</li>
										<li>
												<button onclick="window.location.href=&#39;https://www.xilinx.com/about/company-overview.html&#39;;">Company</button>
												</li>
										</ul>
									<button data-function="close-menu">
										<span class="fal fa-times" aria-hidden="true"></span>
									</button>
								</div>
							</div>
							<div id="header-container-1" class="menu-container">
								<div class="navbar-nav-container">
									<ul class="nav navbar-nav">
										<li>
												<button onclick="window.location.href=&#39;https://www.xilinx.com/applications.html&#39;;">Solutions</button>
												</li>
										<li>
												<button onclick="window.location.href=&#39;https://www.xilinx.com/products/silicon-devices.html&#39;;">Products</button>
												</li>
										<li>
												<button onclick="window.location.href=&#39;https://www.xilinx.com/about/company-overview.html&#39;;">Company</button>
												</li>
										</ul>
									<button data-function="close-menu">
										<span class="fal fa-times" aria-hidden="true"></span>
									</button>
								</div>
							</div>
							<div id="header-container-2" class="menu-container">
								<div class="navbar-nav-container">
									<ul class="nav navbar-nav">
										<li>
												<button onclick="window.location.href=&#39;https://www.xilinx.com/applications.html&#39;;">Solutions</button>
												</li>
										<li>
												<button onclick="window.location.href=&#39;https://www.xilinx.com/products/silicon-devices.html&#39;;">Products</button>
												</li>
										<li>
												<button onclick="window.location.href=&#39;https://www.xilinx.com/about/company-overview.html&#39;;">Company</button>
												</li>
										</ul>
									<button data-function="close-menu">
										<span class="fal fa-times" aria-hidden="true"></span>
									</button>
								</div>
							</div>
							</div>
					<div class="logo-column col-xs-4 col-sm-2">
						<div class="logo">
							<a target="_blank" href="https://www.xilinx.com/">
								<img src="https://github.com/Xilinx/Image-Collateral/blob/main/xilinx-header-logo.svg?raw=true" title="Xilinx Inc"/>
							</a>
						</div>
					</div>

				</div>
			</div>
			</nav></header></noindex></div>
		
	
</div>

    
</div>
</div>

<div class="calloutBanner parbase aem-GridColumn--default--none aem-GridColumn aem-GridColumn--offset--default--0 aem-GridColumn--default--16"><div class="callout-banner">
    Xilinx is now a part of <a target="_blank" href="https://www.amd.com/en/corporate/xilinx-acquisition">AMD</a> |  <a target="_blank" href="https://www.amd.com/en/corporate/privacy">Learn More</a>
</div>
</div>
				<div class="parsys aem-GridColumn--xxxlarge--none aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
						<div class="container-fluid">
							<div class="row">
							<div class="col-xs-12">
   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../../index.html" class="icon icon-home" alt="Documentation Home"> Vitis™ Video Analytics SDK
          

          
          </a>

          
            
            
              <div class="version">
                2.0
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

      
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
            
            
            
              
            
            
              <p class="caption" role="heading"><span class="caption-text">Release Notes</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../release_notes.html">Release Notes</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Base Infrastructure</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../common/common_plugins.html">Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../common/vvas_meta_data_structures.html">Meta Data</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../common/for_developers.html">For Advanced Developers</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Embedded</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../platforms_and_applications.html">Platforms &amp; Applications</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../platforms_and_applications.html#som-examples">SOM Examples</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../platforms_and_applications.html#zcu104-examples">ZCU104 Examples</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../smart_model_select.html">Smart Model Select</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">Multi Channel ML</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#requirements">Requirements</a></li>
<li class="toctree-l4"><a class="reference internal" href="#pre-built-binaries">Pre-built binaries</a></li>
<li class="toctree-l4"><a class="reference internal" href="#example-design-creation">Example Design Creation</a></li>
<li class="toctree-l4"><a class="reference internal" href="#known-issues">Known Issues</a></li>
<li class="toctree-l4"><a class="reference internal" href="#references">References</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../embedded-plugins.html">Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="Tutorials.html">Tutorials</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Data Center</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../DC/platforms_and_applications.html">Platforms &amp; Applications</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../DC/DC_plugins.html">Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../DC/Tutorials/Tutorials.html">Tutorials</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">FAQ</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../freq_asked_questions.html">Frequently Asked Questions</a></li>
</ul>
<p class="caption" role="heading"><span class="caption-text">Other Versions</span></p>
<ul>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/VVAS/1.1/build/html/index.html">1.1</a></li>
<li class="toctree-l1"><a class="reference external" href="https://xilinx.github.io/VVAS/1.0/build/html/index.html">1.0</a></li>
</ul>

            
			
			<p class="caption"><span class="caption-text">This Page</span></p>
				<ul class="current">
				  <li class="toctree-l1"><a href="../../../_sources/docs/Embedded/Tutorials/MultiChannelML.rst.txt"
						rel="nofollow">Show Source</a></li>
				</ul>
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">Vitis™ Video Analytics SDK</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../platforms_and_applications.html">Platforms And Applications</a> &raquo;</li>
        
      <li>Multichannel ML</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../../_sources/docs/Embedded/Tutorials/MultiChannelML.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <section id="multichannel-ml">
<h1>Multichannel ML<a class="headerlink" href="#multichannel-ml" title="Permalink to this heading">¶</a></h1>
<p>This tutorial covers steps to create Machine Learning (ML) based example pipelines. There are two types of Machine Learning pipelines:</p>
<ul class="simple">
<li><p>Single stage ML that involve only one ML operation on the input image</p></li>
<li><p>Multi stage ML, also commonly known as Cascaded ML, that involve several ML operation using different ML models on single input image</p></li>
</ul>
<p>This tutorial begins with building a one stream, single stage Machine Learning pipeline using VVAS and then scales up to build four channel Machine learning pipelines that processes 4 streams in parallel.</p>
<p>The pipeline will run some ML model on the four H.264 decoded streams and mix the videos and display the four streams on HDMI Monitor.</p>
<img alt="../../../_images/four_channel_pipeline.png" class="align-center" src="../../../_images/four_channel_pipeline.png" />
<p>Once single stage ML pipeline creation is completed, this tutorial covers the steps to build multi stage ML pipelines.</p>
<section id="requirements">
<h2>Requirements<a class="headerlink" href="#requirements" title="Permalink to this heading">¶</a></h2>
<section id="hardware-requirements">
<h3>Hardware Requirements<a class="headerlink" href="#hardware-requirements" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p><a class="reference external" href="https://www.xilinx.com/products/boards-and-kits/zcu104.html">ZCU104 Evaluation Board Rev 1.0</a></p></li>
<li><p>Micro USB cable, connected to laptop or desktop computer for the terminal emulator</p></li>
<li><p>MicroSD card, 8 GB or larger, class 10 (recommended)</p></li>
<li><p>HDMI 2.0 supported Monitor with 3840x2160 as the native resolution</p></li>
<li><p>HDMI 2.0 cable</p></li>
</ul>
</section>
<section id="software-requirements">
<h3>Software Requirements<a class="headerlink" href="#software-requirements" title="Permalink to this heading">¶</a></h3>
<p>(Refer <a class="reference external" href="https://docs.xilinx.com/r/en-US/ug1400-vitis-embedded/Installation">Vitis Unified Software Development Platform 2022.1 Documentation</a> for installation instructions)</p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.xilinx.com/support/download/index.html/content/xilinx/en/downloadNav/vitis/2022-1.html">Vitis™ Unified Software Platform</a> version 2022.1</p></li>
<li><p><a class="reference external" href="https://www.xilinx.com/support/download/index.html/content/xilinx/en/downloadNav/embedded-design-tools/2022-1.html">Petalinux tool</a> version 2022.1</p></li>
<li><p>Serial terminal emulator (for example, Tera Term)</p></li>
<li><p>Git</p></li>
<li><p>Host system with Ubuntu 18.04/20.04 (Recommended)</p></li>
<li><p><a class="reference external" href="https://etcher.download/">Balena Etcher flashing tool</a></p></li>
</ul>
</section>
<section id="system-requirements">
<h3>System Requirements<a class="headerlink" href="#system-requirements" title="Permalink to this heading">¶</a></h3>
<ul class="simple">
<li><p>Board must have access to the internet and be accessible from your development system</p></li>
</ul>
</section>
</section>
<section id="pre-built-binaries">
<h2>Pre-built binaries<a class="headerlink" href="#pre-built-binaries" title="Permalink to this heading">¶</a></h2>
<p>Release package COMING SOON provides prebuilt binaries including SD card image that has the implemented design and required software, VAI models and scripts. You may use the pre-built binaries and provided scripts to quickly run the GStreamer pipelines to get a feel of the platform.</p>
<blockquote>
<div><div class="admonition note">
<p class="admonition-title">Note</p>
<p>Pre-built binaries for this example designs will be available soon. You may also build this complete example design following the steps mentioned in <strong>Building Blocks</strong> section. For more information, you may contact <a class="reference external" href="mailto:vvas_discuss&#37;&#52;&#48;amd&#46;com">vvas_discuss<span>&#64;</span>amd<span>&#46;</span>com</a>.</p>
</div>
</div></blockquote>
<p>Download the release package. Let the path where release package is downloaded be represented as <code class="docutils literal notranslate"><span class="pre">&lt;RELEASE_PATH&gt;</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The pre-buit binaries available for download from the link  mentioned above contain software copyrighted by Xilinx and third parties subject to one or more open source software licenses that are contained in the source code files available for download at the link mentioned below.  Please see the source code for the copyright notices and licenses applicable to the software in these binary files.  By downloading these binary files, you agree to abide by the licenses contained in the corresponding source code</p>
</div>
<p>In case user wants to see the Licenses and source code that was used to build these pre-built binaries, download <a class="reference external" href="https://www.xilinx.com/member/forms/download/xef.html?filename=vvas_rel_2_0_thirdparty_sources.zip">Source Licenses and Source Code</a> that contain the Open Source Licenses and source code.</p>
<p>Once you have downloaded the pre-built binaries, you may go to section <a class="reference internal" href="#board-bring-up"><span class="std std-ref">Board bring up</span></a> to try the released SD card image.</p>
<section id="building-blocks">
<h3>Building Blocks<a class="headerlink" href="#building-blocks" title="Permalink to this heading">¶</a></h3>
<p>Let us begin with constructing a single stream video pipeline based on the components selected.</p>
<a class="reference internal image-reference" href="../../../_images/single_channel_pipeline.png"><img alt="../../../_images/single_channel_pipeline.png" class="align-center" src="../../../_images/single_channel_pipeline.png" style="width: 1009.4px; height: 371.7px;" /></a>
<p>We shall build the pipeline incrementally, starting from the source element and keep appending the pipeline per the use case.</p>
<p>First setup the ZCU104 board with steps outlined in <a class="reference internal" href="#board-bring-up"><span class="std std-ref">Board bring up</span></a>.
Facedetect model (densebox_320_320) is used in constructing the single stream pipeline, hence choose a mp4 video file with human faces.</p>
<section id="vcu-decoder-block">
<h4>VCU Decoder block<a class="headerlink" href="#vcu-decoder-block" title="Permalink to this heading">¶</a></h4>
<p>A VCU Decoder block is required to decode the H.264/H.265 encoded stream and feed the decoded data to the ML block for inference.
For good performance, the hardware VCU block is expected to be part of the Xilinx platform.
The <code class="docutils literal notranslate"><span class="pre">zcu104_vcuDec_vmixHdmiTx</span></code> platform provides VCU as a hardware block as part of the design and the omxh264dec plugin for decoding.
Refer to <a class="reference external" href="https://www.xilinx.com/support/documentation/ip_documentation/vcu/v1_2/pg252-vcu.pdf">pg252</a> for more information on the Xilinx VCU block.</p>
<a class="reference internal image-reference" href="../../../_images/omxh264dec_plugin.png"><img alt="../../../_images/omxh264dec_plugin.png" class="align-center" src="../../../_images/omxh264dec_plugin.png" style="width: 949.1999999999999px; height: 205.79999999999998px;" /></a>
<p>VVAS solution for VCU block</p>
<p>Standalone VCU block can be tested with following pipeline:</p>
<figure class="align-center" id="id2">
<a class="reference internal image-reference" href="../../../_images/decode_pipeline.png"><img alt="../../../_images/decode_pipeline.png" src="../../../_images/decode_pipeline.png" style="width: 1010.0999999999999px; height: 371.7px;" /></a>
<figcaption>
<p><span class="caption-text">Sample video pipeline for VCU block</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p><em>GStreamer command</em>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>gst-launch-1.0 filesrc location=/home/root/videos/face_detect.mp4 ! qtdemux ! h264parse ! omxh264dec internal-entropy-buffers=2 ! filesink location=./vcu_out.nv12 -v
</pre></div>
</div>
</section>
<section id="hdmi-tx-block">
<h4>HDMI Tx Block<a class="headerlink" href="#hdmi-tx-block" title="Permalink to this heading">¶</a></h4>
<p>In the previous section, the elementary pipeline is working but the output image from the VCU Decoder block is dump to file via filesink GStreamer opensource plugin.
Now we can replace filesink with the HDMI Tx using kmssink GStreamer plugin.
This enables viewing video on HDMI monitor.
You need to set DRM bus-id, plane-id and rendering position as kmssink properties.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">bus-id</span></code> for the zcu104_vcuDec_vmixHdmiTx platform is fixed to <code class="docutils literal notranslate"><span class="pre">a0130000.v_mix</span></code>.</p>
<p>The video mixer in zcu104_vcuDec_vmixHdmiTx platform supports 9 planes of NV12 format, with plane-id starting from 34 to 42.
You need to set the <code class="docutils literal notranslate"><span class="pre">plane-id</span></code> within this range to output the video stream on one of these planes.</p>
<p>The <code class="docutils literal notranslate"><span class="pre">render-rectangle</span></code> property sets the position of video stream on screen in the format “&lt;x, y, width, height&gt;”.
Here, x, y represents the starting position of the image on screen,
width represents width of the video image, and height represents height of the video image.</p>
<p>Sample video pipeline for adding HDMI Tx is shown as below</p>
<figure class="align-center" id="id3">
<a class="reference internal image-reference" href="../../../_images/decode_hdmitx_pipeline.png"><img alt="../../../_images/decode_hdmitx_pipeline.png" src="../../../_images/decode_hdmitx_pipeline.png" style="width: 1045.8px; height: 371.7px;" /></a>
<figcaption>
<p><span class="caption-text">Sample video pipeline adding Decoder and HDMI Tx blocks</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p><em>GStreamer command</em>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>gst-launch-1.0 -v filesrc location=/home/root/videos/FACEDETECT.mp4 \
      ! qtdemux ! h264parse ! omxh264dec internal-entropy-buffers=2 \
      ! kmssink plane-id=34 bus-id=&quot;a0130000.v_mix&quot;
</pre></div>
</div>
<p>The output can be shift to one corner of the screen by using “render-rectangle” property of kmssink</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>gst-launch-1.0 -v filesrc location=/home/root/videos/FACEDETECT.mp4 \
      ! qtdemux ! h264parse ! omxh264dec internal-entropy-buffers=2 \
      ! kmssink plane-id=34 bus-id=&quot;a0130000.v_mix&quot; render-rectangle=&quot;&lt;0,0,1920,1080&gt;&quot;
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>It is assumed that the video resolution of the input file FACEDETECT.mp4 is 1080P.</p>
</div>
</section>
<section id="machine-learning-ml-block">
<h4>Machine Learning (ML) block<a class="headerlink" href="#machine-learning-ml-block" title="Permalink to this heading">¶</a></h4>
<p>Machine Learning inference is performed by DPU hardware accelerator and <a class="reference internal" href="../../common/common_plugins.html#vvas-xinfer"><span class="std std-ref">vvas_xinfer</span></a> plug-in.
VVAS supports the DPU libraries released with <a class="reference external" href="https://github.com/Xilinx/Vitis-AI">Vitis-AI</a> 2.5. <a class="reference internal" href="../../common/common_plugins.html#vvas-xinfer"><span class="std std-ref">vvas_xinfer</span></a> is used along with the <a class="reference internal" href="../../common/common_plugins.html#vvas-xdpuinfer"><span class="std std-ref">vvas_xdpuinfer</span></a> acceleration software library to perform the Machine Learning Inference.
The beauty of this VVAS solution is that user need not figure out the resolution required for various DPU supported models.
vvas_xinfer plug-in gets this information from the requested model and perform resize, color space conversion operation on the input image as per the requirement of the model using preprocessor block (vvas_xpreprocessor). The output of the vvas_xinfer is the original input image along with the scaled metadata for that resolution.</p>
<p>The information for the ML model to be used must be provided in the JSON file, that is passed to vvas_xfilter’s plug-in property <strong>infer-config</strong>.</p>
<figure class="align-center" id="id4">
<a class="reference internal image-reference" href="../../../_images/xfilter_plugin.png"><img alt="../../../_images/xfilter_plugin.png" src="../../../_images/xfilter_plugin.png" style="width: 695.4px; height: 444.59999999999997px;" /></a>
<figcaption>
<p><span class="caption-text">VVAS solution for ML block</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p>ML block can be tested with following pipeline:</p>
<figure class="align-center" id="id5">
<a class="reference internal image-reference" href="../../../_images/ML_pipeline.png"><img alt="../../../_images/ML_pipeline.png" src="../../../_images/ML_pipeline.png" style="width: 1019.9px; height: 371.7px;" /></a>
<figcaption>
<p><span class="caption-text">Sample Video Pipeline adding ML block</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p><em>GStreamer command</em>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>gst-launch-1.0 -v filesrc location=/home/root/videos/FACEDETECT.mp4 \
  ! qtdemux ! h264parse ! omxh264dec internal-entropy-buffers=2 ! queue \
  ! vvas_xinfer preprocess-config=kernel_pp_facedectect.json infer-config=kernel_densebox_320_320.json name=infer1 ! queue \
  ! kmssink plane-id=34 bus-id=&quot;a0130000.v_mix&quot; render-rectangle=&quot;&lt;0,0,1920,1080&gt;&quot;
</pre></div>
</div>
<p>You can notice that the caps are not mentioned after the decoder as the vvas_xinfer auto negotiates the caps based on the model selected.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In this pipeline, if the debug_level of <code class="docutils literal notranslate"><span class="pre">vvas_xdpuinfer</span></code> library is increased to 2, you can see the objects detected in logs.
The debug level can be increased in the <code class="docutils literal notranslate"><span class="pre">kernel_densebox_320_320.json</span></code> JSON file.
The sample log output is shown below.</p>
<figure class="align-center">
<a class="reference internal image-reference" href="../../../_images/inference_result_dump.png"><img alt="../../../_images/inference_result_dump.png" src="../../../_images/inference_result_dump.png" style="width: 502.5px; height: 314.0px;" /></a>
</figure>
</div>
<p>Sample JSON files <strong>kernel_pp_facedectect.json</strong> for preprocesing and <strong>kernel_densebox_320_320.json</strong> for densebox_320_320 DPU model for detection of a human face are provided for reference.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
  <span class="s2">&quot;xclbin-location&quot;</span><span class="p">:</span><span class="s2">&quot;/media/sd-mmcblk0p1/dpu.xclbin&quot;</span><span class="p">,</span>
  <span class="s2">&quot;vvas-library-repo&quot;</span><span class="p">:</span> <span class="s2">&quot;/usr/lib&quot;</span><span class="p">,</span>
  <span class="s2">&quot;device-index&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
  <span class="s2">&quot;kernels&quot;</span> <span class="p">:[</span>
    <span class="p">{</span>
      <span class="s2">&quot;kernel-name&quot;</span><span class="p">:</span><span class="s2">&quot;v_multi_scaler:</span><span class="si">{v_multi_scaler_1}</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="s2">&quot;library-name&quot;</span><span class="p">:</span> <span class="s2">&quot;libvvas_xpreprocessor.so&quot;</span><span class="p">,</span>
      <span class="s2">&quot;config&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;alpha_r&quot;</span> <span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
        <span class="s2">&quot;alpha_g&quot;</span> <span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
        <span class="s2">&quot;alpha_b&quot;</span> <span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
        <span class="s2">&quot;beta_r&quot;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;beta_g&quot;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;beta_b&quot;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;inference-level&quot;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;debug_level&quot;</span> <span class="p">:</span> <span class="mi">0</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">]</span>
<span class="p">}</span>
<span class="n">kernel_pp_facedectect</span><span class="o">.</span><span class="n">json</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
  <span class="s2">&quot;vvas-library-repo&quot;</span><span class="p">:</span> <span class="s2">&quot;/usr/lib/&quot;</span><span class="p">,</span>
  <span class="s2">&quot;inference-level&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
  <span class="s2">&quot;attach-ppe-outbuf&quot;</span><span class="p">:</span> <span class="n">false</span><span class="p">,</span>
  <span class="s2">&quot;kernel&quot;</span> <span class="p">:</span> <span class="p">{</span>
     <span class="s2">&quot;library-name&quot;</span><span class="p">:</span><span class="s2">&quot;libvvas_xdpuinfer.so&quot;</span><span class="p">,</span>
     <span class="s2">&quot;config&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;batch-size&quot;</span> <span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="s2">&quot;model-name&quot;</span> <span class="p">:</span> <span class="s2">&quot;densebox_320_320&quot;</span><span class="p">,</span>
        <span class="s2">&quot;model-class&quot;</span> <span class="p">:</span> <span class="s2">&quot;FACEDETECT&quot;</span><span class="p">,</span>
        <span class="s2">&quot;model-format&quot;</span> <span class="p">:</span> <span class="s2">&quot;BGR&quot;</span><span class="p">,</span>
        <span class="s2">&quot;model-path&quot;</span> <span class="p">:</span> <span class="s2">&quot;/usr/share/vitis_ai_library/models/&quot;</span><span class="p">,</span>
        <span class="s2">&quot;run_time_model&quot;</span> <span class="p">:</span> <span class="n">false</span><span class="p">,</span>
        <span class="s2">&quot;need_preprocess&quot;</span> <span class="p">:</span> <span class="n">false</span><span class="p">,</span>
        <span class="s2">&quot;performance_test&quot;</span> <span class="p">:</span> <span class="n">false</span><span class="p">,</span>
        <span class="s2">&quot;debug_level&quot;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;max-objects&quot;</span><span class="p">:</span><span class="mi">3</span>
     <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
<span class="n">kernel_densebox_320_320</span><span class="o">.</span><span class="n">json</span>
</pre></div>
</div>
<p>Different ML models supported by the DPU have different preprocessing requirements that can include resize, mean subtraction, scale normalization etc. Additionally, the DPU expects input image in BGR/RGB format. The VCU decoder at the input of the DPU generates NV12 images. Depending on the model selected, the preprocessor block is expected to support the following operations:</p>
<ul class="simple">
<li><p>Resize</p></li>
<li><p>Color space conversion</p></li>
<li><p>Mean Subtraction</p></li>
<li><p>Scale Normalization</p></li>
</ul>
<p>Although all these operations can be achieved in software, the performance impact is substantial. VVAS support hardware accelerated pre-processing. Configuration parameters for pre-processing block can be specified through a json file by providing location in preprocess-config property of <code class="docutils literal notranslate"><span class="pre">vvas_xinfer</span></code> GStreamer plugin.</p>
<p>Table 1 lists the pre-processing parameters supported by <code class="docutils literal notranslate"><span class="pre">vvas_xinfer</span></code> GStreamer plug-in.
These properties are tested in the context of this tutorial only.</p>
<p>Table 1: preprocessing parameters in JSON format to configure mean and scale values</p>
<table class="docutils align-default">
<colgroup>
<col style="width: 27%" />
<col style="width: 18%" />
<col style="width: 15%" />
<col style="width: 18%" />
<col style="width: 23%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p><strong>Property Name</strong></p></th>
<th class="head"><p><strong>Type</strong></p></th>
<th class="head"><p><strong>Range</strong></p></th>
<th class="head"><p><strong>Default</strong></p></th>
<th class="head"><p><strong>Description</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>alpha-b</p></td>
<td><p>float</p></td>
<td><p>0 to 128</p></td>
<td><p>0</p></td>
<td><p>Mean
subtraction for
blue channel</p></td>
</tr>
<tr class="row-odd"><td><p>alpha-g</p></td>
<td><p>float</p></td>
<td><p>0 to 128</p></td>
<td><p>0</p></td>
<td><p>Mean
subtraction for
green channel</p></td>
</tr>
<tr class="row-even"><td><p>alpha-r</p></td>
<td><p>float</p></td>
<td><p>0 to 128</p></td>
<td><p>0</p></td>
<td><p>Mean
subtraction for
red channel</p></td>
</tr>
<tr class="row-odd"><td><p>beta-b</p></td>
<td><p>float</p></td>
<td><p>0 to 1</p></td>
<td><p>1</p></td>
<td><p>Scaling
for blue
channel</p></td>
</tr>
<tr class="row-even"><td><p>beta-g</p></td>
<td><p>float</p></td>
<td><p>0 to 1</p></td>
<td><p>1</p></td>
<td><p>Scaling
for green
channel</p></td>
</tr>
<tr class="row-odd"><td><p>beta-r</p></td>
<td><p>float</p></td>
<td><p>0 to 1</p></td>
<td><p>1</p></td>
<td><p>Scaling
for red
channel</p></td>
</tr>
</tbody>
</table>
<p>Once the objects are detected, you can move to the next advanced blocks.</p>
</section>
<section id="machine-learning-with-preprocessing-in-software">
<h4>Machine Learning with preprocessing in software<a class="headerlink" href="#machine-learning-with-preprocessing-in-software" title="Permalink to this heading">¶</a></h4>
<p>VVAS can also be used on the Platform that may not have hardware accelerated pre-processing (multiscaler kernel) due to any reason. In this case the preprocessing needs to be performed in software. The scaling and color space conversation are done by open source gstremaer plugins and the normalization and scaling are done by Vitis AI library.</p>
<p>Below is the pipe pile without vvas preprocessor.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>gst-launch-1.0 -v filesrc location=/home/root/videos/FACEDETECT.mp4 \
  ! qtdemux ! h264parse ! omxh264dec internal-entropy-buffers=2 ! queue \
  ! videoscale ! queue \
  ! videoconvert ! queue \
  ! vvas_xinfer infer-config=kernel_densebox_320_320.json name=infer1 ! queue \
  ! videoscale ! queue \
  ! videoconvert ! queue \
  ! video/x-raw, width=1920, height=1080, format=NV12 \
  ! kmssink plane-id=34 bus-id=&quot;a0130000.v_mix&quot; render-rectangle=&quot;&lt;0,0,1920,1080&gt;&quot;
</pre></div>
</div>
<p>The following is sample JSON kernel_densebox_320_320.json for running the densebox_320_320 DPU model that detects a human face.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
  <span class="s2">&quot;vvas-library-repo&quot;</span><span class="p">:</span> <span class="s2">&quot;/usr/lib/&quot;</span><span class="p">,</span>
  <span class="s2">&quot;inference-level&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
  <span class="s2">&quot;attach-ppe-outbuf&quot;</span><span class="p">:</span> <span class="n">false</span><span class="p">,</span>
  <span class="s2">&quot;kernel&quot;</span> <span class="p">:</span> <span class="p">{</span>
     <span class="s2">&quot;library-name&quot;</span><span class="p">:</span><span class="s2">&quot;libvvas_xdpuinfer.so&quot;</span><span class="p">,</span>
     <span class="s2">&quot;config&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;batch-size&quot;</span> <span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="s2">&quot;model-name&quot;</span> <span class="p">:</span> <span class="s2">&quot;densebox_320_320&quot;</span><span class="p">,</span>
        <span class="s2">&quot;model-class&quot;</span> <span class="p">:</span> <span class="s2">&quot;FACEDETECT&quot;</span><span class="p">,</span>
        <span class="s2">&quot;model-format&quot;</span> <span class="p">:</span> <span class="s2">&quot;BGR&quot;</span><span class="p">,</span>
        <span class="s2">&quot;model-path&quot;</span> <span class="p">:</span> <span class="s2">&quot;/usr/share/vitis_ai_library/models/&quot;</span><span class="p">,</span>
        <span class="s2">&quot;run_time_model&quot;</span> <span class="p">:</span> <span class="n">false</span><span class="p">,</span>
        <span class="s2">&quot;need_preprocess&quot;</span> <span class="p">:</span> <span class="n">true</span><span class="p">,</span>
        <span class="s2">&quot;performance_test&quot;</span> <span class="p">:</span> <span class="n">false</span><span class="p">,</span>
        <span class="s2">&quot;debug_level&quot;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;max-objects&quot;</span><span class="p">:</span><span class="mi">3</span>
     <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="n">kernel_densebox_320_320</span><span class="o">.</span><span class="n">json</span>
</pre></div>
</div>
<p>You can observe that in above pipeline <strong>preprocess-config</strong> property of <code class="docutils literal notranslate"><span class="pre">vvas_xinfer</span></code> plug-in is not set. This means we do not want to use hardware accelerated pre-processing block of <code class="docutils literal notranslate"><span class="pre">vvas_xinfer</span></code> plug-in and the videoscale and videoconvert GStreamer opensource plug-ins are used to convert the format and colour of input image as required by DPU model and Kmssink. The caps are not mentioned before <code class="docutils literal notranslate"><span class="pre">vvas_xinfer</span></code> and after the decoder as <code class="docutils literal notranslate"><span class="pre">vvas_xinfer</span></code> auto negotiates the caps based on the model selected.</p>
<p>Since we want Vitis AI library to perform the required pre-processing in software, we need to set <strong>need_preprocess</strong> to true in <strong>kernel_densebox_320_320.json</strong>.</p>
<p>Although all these operations can be achieved in software, the performance impact is substantial. So rest of the document consider that the hardware accelerated pre-processing (using multiscaler kernel) is part of the provided hardware.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Though you may not observe any ML Inference information on monitor with this pipeline,
but we should see the input image getting displayed in monitor by this pipeline.</p>
</div>
</section>
<section id="bounding-box-block">
<h4>Bounding Box block<a class="headerlink" href="#bounding-box-block" title="Permalink to this heading">¶</a></h4>
<p>To view the result of ML Inference displayed on the monitor, you should draw the results on an image.
The <a class="reference internal" href="../../common/common_plugins.html#vvas-xboundingbox"><span class="std std-ref">vvas_xboundingbox</span></a> software acceleration library comes in handy in this case.
This library along with VVAS infrastructure plug-in <a class="reference internal" href="../../common/common_plugins.html#vvas-xfilter"><span class="std std-ref">vvas_xfilter</span></a> can provide the bounding box functionality.</p>
<p>Sample video pipeline for adding bounding box block is shown as below</p>
<figure class="align-center" id="id6">
<a class="reference internal image-reference" href="../../../_images/single_channel_pipeline.png"><img alt="../../../_images/single_channel_pipeline.png" src="../../../_images/single_channel_pipeline.png" style="width: 1009.4px; height: 371.7px;" /></a>
<figcaption>
<p><span class="caption-text">Sample Video Pipeline adding Bounding Box block</span><a class="headerlink" href="#id6" title="Permalink to this image">¶</a></p>
</figcaption>
</figure>
<p><em>GStreamer command</em>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>gst-launch-1.0 -v filesrc location=/home/root/videos/FACEDETECT.mp4 \
  ! qtdemux ! h264parse ! omxh264dec internal-entropy-buffers=2 ! queue \
  ! vvas_xinfer preprocess-config=kernel_pp_facedectect.json infer-config=kernel_densebox_320_320.json name=infer1 ! queue \
  ! vvas_xfilter kernels-config=&quot;kernel_boundingbox.json&quot; ! queue \
  ! kmssink plane-id=34 bus-id=&quot;a0130000.v_mix&quot; render-rectangle=&quot;&lt;0,0,1920,1080&gt;&quot;
</pre></div>
</div>
<p>The following sample JSON file kernel_boundingbox.json is used to draw a bounding box on detected objects.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
  <span class="s2">&quot;vvas-library-repo&quot;</span><span class="p">:</span> <span class="s2">&quot;/usr/lib/&quot;</span><span class="p">,</span>
  <span class="s2">&quot;element-mode&quot;</span><span class="p">:</span><span class="s2">&quot;inplace&quot;</span><span class="p">,</span>
  <span class="s2">&quot;kernels&quot;</span> <span class="p">:[</span>
    <span class="p">{</span>
      <span class="s2">&quot;library-name&quot;</span><span class="p">:</span><span class="s2">&quot;libvvas_xboundingbox.so&quot;</span><span class="p">,</span>
      <span class="s2">&quot;config&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;model-name&quot;</span> <span class="p">:</span> <span class="s2">&quot;densebox_320_320&quot;</span><span class="p">,</span>
        <span class="s2">&quot;display_output&quot;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;font_size&quot;</span> <span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
        <span class="s2">&quot;font&quot;</span> <span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="s2">&quot;thickness&quot;</span> <span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="s2">&quot;debug_level&quot;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;label_color&quot;</span> <span class="p">:</span> <span class="p">{</span> <span class="s2">&quot;blue&quot;</span> <span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;green&quot;</span> <span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;red&quot;</span> <span class="p">:</span> <span class="mi">0</span> <span class="p">},</span>
        <span class="s2">&quot;label_filter&quot;</span> <span class="p">:</span> <span class="p">[</span> <span class="s2">&quot;class&quot;</span><span class="p">,</span> <span class="s2">&quot;probability&quot;</span> <span class="p">],</span>
        <span class="s2">&quot;classes&quot;</span> <span class="p">:</span> <span class="p">[</span>
        <span class="p">]</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
<p>With addition of bounding box, your pipeline for single stream is complete.</p>
</section>
</section>
<section id="four-channel-ml-pipeline">
<h3>Four Channel ML pipeline<a class="headerlink" href="#four-channel-ml-pipeline" title="Permalink to this heading">¶</a></h3>
<p>Now, constructing a four-channel pipeline is simply duplicating the above pipeline four times for different models
and positioning each output video appropriately on screen on different plane-ids.</p>
<p>Below Vitis AI models are used as example in this tutorial.
Refer <a class="reference external" href="https://docs.xilinx.com/r/en-US/ug1414-vitis-ai/Compiling-the-Model?tocId=iw~3MFuL5ebBYiu0WFiv~Q">Vitis AI User Documentation</a> to compile different models
using arch.json file from release package.</p>
<ul class="simple">
<li><p>densebox_320_320 (Face detection)</p></li>
<li><p>yolov3_adas_pruned_0_9 (Object detection)</p></li>
<li><p>resnet50 (Classification)</p></li>
<li><p>refinedet_pruned_0_96 (Pedestrian detector)</p></li>
</ul>
<p>A reference pipeline for four channel ML is given below.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>gst-launch-1.0 -v \
 filesrc location=/home/root/videos/FACEDETECT.mp4 \
  ! qtdemux ! h264parse ! omxh264dec internal-entropy-buffers=2 ! queue \
  ! vvas_xinfer preprocess-config=kernel_pp_facedectect.json infer-config=kernel_densebox_320_320.json name=infer1 ! queue \
  ! vvas_xfilter kernels-config=&quot;kernel_boundingbox.json&quot; ! queue \
  ! kmssink plane-id=34 bus-id=&quot;a0130000.v_mix&quot; render-rectangle=&quot;&lt;0,0,1920,1080&gt;&quot; \
filesrc location=/home/root/videos/YOLOV3.mp4 \
  ! qtdemux ! h264parse ! omxh264dec internal-entropy-buffers=2 ! queue \
  ! vvas_xinfer preprocess-config=kernel_pp_yolov3.json infer-config=kernel_yolov3_adas_pruned_0_9.json name=infer2 ! queue \
  ! vvas_xfilter kernels-config=&quot;kernel_boundingbox.json&quot; ! queue \
  ! kmssink plane-id=35 bus-id=&quot;a0130000.v_mix&quot; render-rectangle=&quot;&lt;1920,0,1920,1080&gt;&quot;
filesrc location=/home/root/videos/CLASSIFICATION.mp4 \
  ! qtdemux ! h264parse ! omxh264dec internal-entropy-buffers=2 ! queue \
  ! vvas_xinfer preprocess-config=kernel_pp_resnet50.json infer-config=kernel_resnet50.json name=infer3 ! queue \
  ! vvas_xfilter kernels-config=&quot;kernel_boundingbox.json&quot; ! queue \
  ! kmssink plane-id=36 bus-id=&quot;a0130000.v_mix&quot; render-rectangle=&quot;&lt;0,1080,1920,1080&gt;&quot;
filesrc location=/home/root/videos/REFINEDET.mp4 \
  ! qtdemux ! h264parse ! omxh264dec internal-entropy-buffers=2 ! queue \
  ! vvas_xinfer preprocess-config=kernel_pp_refinedet.json infer-config=kernel_refinedet_pruned_0_96.json name=infer4 ! queue \
  ! vvas_xfilter kernels-config=&quot;kernel_boundingbox.json&quot; ! queue \
  ! kmssink plane-id=37 bus-id=&quot;a0130000.v_mix&quot; render-rectangle=&quot;&lt;1920,1080,1920,1080&gt;&quot;
</pre></div>
</div>
<p>The above command is available in the release package as <code class="docutils literal notranslate"><span class="pre">multichannel_ml.sh</span></code>.</p>
</section>
<section id="vvas-cascaded-machine-learning-usecase">
<h3>VVAS Cascaded Machine Learning usecase<a class="headerlink" href="#vvas-cascaded-machine-learning-usecase" title="Permalink to this heading">¶</a></h3>
<p>There might be multiple use cases where the user wants to process inference on the required portion of image only and not on full image. One of the examples is to detect the faces of people sitting inside a car. In this case, if multiple cars are present in frame, the system must identify the car by some parameters like car number plate, color or make of the car. This information is needed to process the face of the person inside the car.
The scenario mentioned above required multiple levels of ML inference operation in serial or cascade manner where the following inference block works only on the output of the previous inference block.</p>
<p>This tutorial demonstrates how to build such types of use cases using VVAS with minimal configuration and with ease.</p>
<p>In this tutorial, the end goal is to figure out the plate number of Cars in the frame. We will be using the <code class="docutils literal notranslate"><span class="pre">plate</span> <span class="pre">num</span></code> model to detect the number plate. This model expects image that has number in it, no extra border. So, to feed the image of the number plate to the <code class="docutils literal notranslate"><span class="pre">plate</span> <span class="pre">num</span></code> model one should crop the plate from the frame and provide it to <code class="docutils literal notranslate"><span class="pre">plate</span> <span class="pre">num</span></code> model after doing Mean Subtraction and Scale Normalization. So, to achieve this use case 3 levels of ML inference operations are performed. First level ML inference detect the cars in the frame, 2nd level detects the number plate in the provided image of the car and 3rd level finds the number in the plate.</p>
<p>Below diagram express the use case mentioned above.</p>
<img alt="../../../_images/plate_detect_usecase.png" class="align-center" src="../../../_images/plate_detect_usecase.png" />
<p>By the end of this tutorial, you should be able to build and run the following pipeline.</p>
<img alt="../../../_images/cascase1_pipeline.png" class="align-center" src="../../../_images/cascase1_pipeline.png" />
</section>
<section id="cascade-building-blocks">
<h3>Cascade Building Blocks<a class="headerlink" href="#cascade-building-blocks" title="Permalink to this heading">¶</a></h3>
<p>Different ML models supported by the DPU have different preprocessing requirements that can include resize, mean subtraction, scale normalization etc. Additionally, the DPU expects input images in BGR/RGB formats. The VCU decoder at the input of the DPU generates NV12 images. Depending on the model selected, the preprocessor block is expected to support the following operations:</p>
<ul class="simple">
<li><p>Resize</p></li>
<li><p>Color space conversion</p></li>
<li><p>Mean Subtraction</p></li>
<li><p>Scale Normalization</p></li>
</ul>
<p>Let us begin with constructing incremental video pipeline based on the components selected.</p>
<section id="first-level-inference">
<h4>First Level inference<a class="headerlink" href="#first-level-inference" title="Permalink to this heading">¶</a></h4>
<p>We start to add the first level of ML inference that will detect the cars. This can be achieved using <code class="docutils literal notranslate"><span class="pre">yolov3_voc</span></code> model.</p>
<img alt="../../../_images/cascase_1st_level_pipeline.png" class="align-center" src="../../../_images/cascase_1st_level_pipeline.png" />
<p>Pipeline to demonstrate the car detection from frame and display output to monitor is as mentioned below.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>gst-launch-1.0 -v  \
  filesrc location=/home/root/videos/platedetect_sample.mp4   \
   ! qtdemux ! h264parse ! omxh264dec internal-entropy-buffers=2 ! queue  \
   ! vvas_xinfer preprocess-config=kernel_pp_yolov3.json infer-config=kernel_yolov3_voc.json name=infer1 ! queue  \
   ! vvas_xfilter kernels-config=&quot;kernel_boundingbox.json&quot; ! queue  \
   ! kmssink plane-id=34 bus-id=&quot;a0130000.v_mix&quot;
</pre></div>
</div>
<p>Below are the sample json files.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
  <span class="s2">&quot;xclbin-location&quot;</span><span class="p">:</span><span class="s2">&quot;/media/sd-mmcblk0p1/dpu.xclbin&quot;</span><span class="p">,</span>
  <span class="s2">&quot;vvas-library-repo&quot;</span><span class="p">:</span> <span class="s2">&quot;/usr/lib&quot;</span><span class="p">,</span>
  <span class="s2">&quot;device-index&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
  <span class="s2">&quot;kernels&quot;</span> <span class="p">:[</span>
    <span class="p">{</span>
      <span class="s2">&quot;kernel-name&quot;</span><span class="p">:</span><span class="s2">&quot;v_multi_scaler:</span><span class="si">{v_multi_scaler_1}</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="s2">&quot;library-name&quot;</span><span class="p">:</span> <span class="s2">&quot;libvvas_xpreprocessor.so&quot;</span><span class="p">,</span>
      <span class="s2">&quot;config&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;alpha_r&quot;</span> <span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="s2">&quot;alpha_g&quot;</span> <span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="s2">&quot;alpha_b&quot;</span> <span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
        <span class="s2">&quot;beta_r&quot;</span> <span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span>
        <span class="s2">&quot;beta_g&quot;</span> <span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span>
        <span class="s2">&quot;beta_b&quot;</span> <span class="p">:</span> <span class="mf">0.25</span><span class="p">,</span>
        <span class="s2">&quot;inference-level&quot;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;debug_level&quot;</span> <span class="p">:</span> <span class="mi">0</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">]</span>
<span class="p">}</span>

<span class="n">kernel_pp_yolov3</span><span class="o">.</span><span class="n">json</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
  <span class="s2">&quot;vvas-library-repo&quot;</span><span class="p">:</span> <span class="s2">&quot;/usr/lib/&quot;</span><span class="p">,</span>
  <span class="s2">&quot;inference-level&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
  <span class="s2">&quot;attach-ppe-outbuf&quot;</span><span class="p">:</span> <span class="n">false</span><span class="p">,</span>
  <span class="s2">&quot;kernel&quot;</span> <span class="p">:</span> <span class="p">{</span>
    <span class="s2">&quot;library-name&quot;</span><span class="p">:</span><span class="s2">&quot;libvvas_xdpuinfer.so&quot;</span><span class="p">,</span>
    <span class="s2">&quot;config&quot;</span><span class="p">:</span> <span class="p">{</span>
      <span class="s2">&quot;batch-size&quot;</span> <span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
      <span class="s2">&quot;model-name&quot;</span> <span class="p">:</span> <span class="s2">&quot;yolov3_voc&quot;</span><span class="p">,</span>
      <span class="s2">&quot;model-class&quot;</span> <span class="p">:</span> <span class="s2">&quot;YOLOV3&quot;</span><span class="p">,</span>
      <span class="s2">&quot;model-format&quot;</span> <span class="p">:</span> <span class="s2">&quot;RGB&quot;</span><span class="p">,</span>
      <span class="s2">&quot;model-path&quot;</span> <span class="p">:</span> <span class="s2">&quot;/usr/share/vitis_ai_library/models/&quot;</span><span class="p">,</span>
      <span class="s2">&quot;run_time_model&quot;</span> <span class="p">:</span> <span class="n">false</span><span class="p">,</span>
      <span class="s2">&quot;need_preprocess&quot;</span> <span class="p">:</span> <span class="n">false</span><span class="p">,</span>
      <span class="s2">&quot;performance_test&quot;</span> <span class="p">:</span> <span class="n">false</span><span class="p">,</span>
      <span class="s2">&quot;debug_level&quot;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
      <span class="s2">&quot;max-objects&quot;</span><span class="p">:</span><span class="mi">3</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="n">kernel_yolov3_voc</span><span class="o">.</span><span class="n">json</span>
</pre></div>
</div>
<p>Here we need to understand the complexity which is taken care of by the VVAS framework in a very easy user interface. The output of VCU Decoder is 1920X1080 <code class="docutils literal notranslate"><span class="pre">NV12</span></code> and the requirement for <code class="docutils literal notranslate"><span class="pre">yolov3_voc</span></code> is 360X360 <code class="docutils literal notranslate"><span class="pre">RGB</span></code>. This conversion is taken care of by the preprocessor block which is part of <code class="docutils literal notranslate"><span class="pre">vvas_xinfer</span></code> plugin. Not only the color and format conversion, the preprocessor block also does Mean Subtraction and Scale Normalization. Although all these operations can be achieved in software, the performance impact is substantial.</p>
<p>For simplicity, a common json file is used for bounding box. Please refer <a class="reference internal" href="../../common/common_plugins.html#vvas-xboundingbox"><span class="std std-ref">vvas_xboundingbox</span></a> for more detailed parameters of bounding box.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
  <span class="s2">&quot;vvas-library-repo&quot;</span><span class="p">:</span> <span class="s2">&quot;/usr/lib/&quot;</span><span class="p">,</span>
  <span class="s2">&quot;element-mode&quot;</span><span class="p">:</span><span class="s2">&quot;inplace&quot;</span><span class="p">,</span>
  <span class="s2">&quot;kernels&quot;</span> <span class="p">:[</span>
    <span class="p">{</span>
      <span class="s2">&quot;library-name&quot;</span><span class="p">:</span><span class="s2">&quot;libvvas_xboundingbox.so&quot;</span><span class="p">,</span>
      <span class="s2">&quot;config&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;model-name&quot;</span> <span class="p">:</span> <span class="s2">&quot;densebox_320_320&quot;</span><span class="p">,</span>
        <span class="s2">&quot;display_output&quot;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;font_size&quot;</span> <span class="p">:</span> <span class="mf">0.5</span><span class="p">,</span>
        <span class="s2">&quot;font&quot;</span> <span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="s2">&quot;thickness&quot;</span> <span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="s2">&quot;debug_level&quot;</span> <span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="s2">&quot;label_color&quot;</span> <span class="p">:</span> <span class="p">{</span> <span class="s2">&quot;blue&quot;</span> <span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;green&quot;</span> <span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;red&quot;</span> <span class="p">:</span> <span class="mi">0</span> <span class="p">},</span>
        <span class="s2">&quot;label_filter&quot;</span> <span class="p">:</span> <span class="p">[</span> <span class="s2">&quot;class&quot;</span><span class="p">,</span> <span class="s2">&quot;probability&quot;</span> <span class="p">],</span>
        <span class="s2">&quot;classes&quot;</span> <span class="p">:</span> <span class="p">[</span>
        <span class="p">]</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">]</span>
<span class="p">}</span>

<span class="n">kernel_boundingbox</span><span class="o">.</span><span class="n">json</span>
</pre></div>
</div>
</section>
<section id="second-level-inference">
<h4>Second Level inference<a class="headerlink" href="#second-level-inference" title="Permalink to this heading">¶</a></h4>
<p>First level inference detects the car in the frame, now we need to find the number plate in the area where the car is detected. So, lets add second level ML Inference with <code class="docutils literal notranslate"><span class="pre">plate</span> <span class="pre">detect</span></code> model just after the first level ML Inference with <code class="docutils literal notranslate"><span class="pre">yolov3_voc</span></code> model.</p>
<p>Below is the GStreamer pipe demonstrating the number plate detect after car detect and display output to monitor using the kmssink plugin.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>gst-launch-1.0 -v  \
 filesrc location=/home/root/videos/platedetect_sample.mp4   \
  ! qtdemux ! h264parse ! omxh264dec internal-entropy-buffers=2 ! queue  \
  ! vvas_xinfer preprocess-config=kernel_pp_yolov3.json infer-config=kernel_yolov3_voc.json name=infer1 ! queue  \
  ! vvas_xinfer preprocess-config=kernel_pp_platedetect.json infer-config=kernel_platedetect.json name=infer2 ! queue  \
  ! vvas_xfilter kernels-config=&quot;kernel_boundingbox.json&quot; ! queue  \
  ! kmssink plane-id=34 bus-id=&quot;a0130000.v_mix&quot;
</pre></div>
</div>
<p>Below are the sample json files for 2nd level.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
  <span class="s2">&quot;xclbin-location&quot;</span><span class="p">:</span><span class="s2">&quot;/media/sd-mmcblk0p1/dpu.xclbin&quot;</span><span class="p">,</span>
  <span class="s2">&quot;vvas-library-repo&quot;</span><span class="p">:</span> <span class="s2">&quot;/usr/lib&quot;</span><span class="p">,</span>
  <span class="s2">&quot;device-index&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
  <span class="s2">&quot;kernels&quot;</span> <span class="p">:[</span>
    <span class="p">{</span>
      <span class="s2">&quot;kernel-name&quot;</span><span class="p">:</span><span class="s2">&quot;v_multi_scaler:</span><span class="si">{v_multi_scaler_1}</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="s2">&quot;library-name&quot;</span><span class="p">:</span> <span class="s2">&quot;libvvas_xpreprocessor.so&quot;</span><span class="p">,</span>
      <span class="s2">&quot;config&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;alpha_r&quot;</span> <span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
        <span class="s2">&quot;alpha_g&quot;</span> <span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
        <span class="s2">&quot;alpha_b&quot;</span> <span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
        <span class="s2">&quot;beta_r&quot;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;beta_g&quot;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;beta_b&quot;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;inference-level&quot;</span> <span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
        <span class="s2">&quot;debug_level&quot;</span> <span class="p">:</span> <span class="mi">0</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">]</span>
<span class="p">}</span>

<span class="n">kernel_pp_platedetect</span><span class="o">.</span><span class="n">json</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
  <span class="s2">&quot;vvas-library-repo&quot;</span><span class="p">:</span> <span class="s2">&quot;/usr/lib/&quot;</span><span class="p">,</span>
  <span class="s2">&quot;inference-level&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span>
  <span class="s2">&quot;attach-ppe-outbuf&quot;</span><span class="p">:</span> <span class="n">false</span><span class="p">,</span>
  <span class="s2">&quot;kernel&quot;</span> <span class="p">:</span> <span class="p">{</span>
    <span class="s2">&quot;library-name&quot;</span><span class="p">:</span><span class="s2">&quot;libvvas_xdpuinfer.so&quot;</span><span class="p">,</span>
    <span class="s2">&quot;config&quot;</span><span class="p">:</span> <span class="p">{</span>
      <span class="s2">&quot;batch-size&quot;</span> <span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
      <span class="s2">&quot;model-name&quot;</span> <span class="p">:</span> <span class="s2">&quot;plate_detect&quot;</span><span class="p">,</span>
      <span class="s2">&quot;model-class&quot;</span> <span class="p">:</span> <span class="s2">&quot;PLATEDETECT&quot;</span><span class="p">,</span>
      <span class="s2">&quot;model-format&quot;</span> <span class="p">:</span> <span class="s2">&quot;BGR&quot;</span><span class="p">,</span>
      <span class="s2">&quot;model-path&quot;</span> <span class="p">:</span> <span class="s2">&quot;/usr/share/vitis_ai_library/models/&quot;</span><span class="p">,</span>
      <span class="s2">&quot;run_time_model&quot;</span> <span class="p">:</span> <span class="n">false</span><span class="p">,</span>
      <span class="s2">&quot;need_preprocess&quot;</span> <span class="p">:</span> <span class="n">false</span><span class="p">,</span>
      <span class="s2">&quot;performance_test&quot;</span> <span class="p">:</span> <span class="n">false</span><span class="p">,</span>
      <span class="s2">&quot;debug_level&quot;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
      <span class="s2">&quot;max-objects&quot;</span><span class="p">:</span><span class="mi">3</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="n">kernel_platedetect</span><span class="o">.</span><span class="n">json</span>
</pre></div>
</div>
<p>Please note the “inference-level” parameter in both the json is 2 which tells the framework that this model is placed at level 2 in full use case.
As we discussed, there might be multiple cars in frame and we need to find the number plate for each of them so when the image, along with the metadata detected in first level reaches 2nd- level, the pre-processing stage at 2nd level inference first crops the car found in first level and scale down to format/resolution required by <code class="docutils literal notranslate"><span class="pre">plate</span> <span class="pre">detect</span></code> model. All this cropping and scaling is done by preprocessor block without user know about it.</p>
<p>Similarly, when data passes to 3rd level, vvas framework reads the metadata and crop the number plate from full image, scale to required format and pass it to <code class="docutils literal notranslate"><span class="pre">plate</span> <span class="pre">number</span></code> model which find the number inside the image provided to model.</p>
</section>
<section id="third-level-inference">
<h4>Third Level inference<a class="headerlink" href="#third-level-inference" title="Permalink to this heading">¶</a></h4>
<p>Below is the full GStreamer pipe demonstrating the number plate detect and display using the kmssink plugin.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>gst-launch-1.0 -v  \
 filesrc location=/home/root/videos/platedetect_sample.mp4   \
  ! qtdemux ! h264parse ! omxh264dec internal-entropy-buffers=2 ! queue  \
  ! vvas_xinfer preprocess-config=kernel_pp_yolov3.json infer-config=kernel_yolov3_voc.json name=infer1 ! queue  \
  ! vvas_xinfer preprocess-config=kernel_pp_platedetect.json infer-config=kernel_platedetect.json name=infer2 ! queue  \
  ! vvas_xinfer preprocess-config=kernel_pp_plate_num.json infer-config=kernel_plate_num.json name=infer3 ! queue  \
  ! vvas_xfilter kernels-config=&quot;kernel_boundingbox.json&quot; ! queue  \
  ! kmssink plane-id=34 bus-id=&quot;a0130000.v_mix&quot;
</pre></div>
</div>
<p>Below are the sample json files for 3rd level.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
  <span class="s2">&quot;xclbin-location&quot;</span><span class="p">:</span><span class="s2">&quot;/media/sd-mmcblk0p1/dpu.xclbin&quot;</span><span class="p">,</span>
  <span class="s2">&quot;vvas-library-repo&quot;</span><span class="p">:</span> <span class="s2">&quot;/usr/lib&quot;</span><span class="p">,</span>
  <span class="s2">&quot;device-index&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
  <span class="s2">&quot;kernels&quot;</span> <span class="p">:[</span>
    <span class="p">{</span>
      <span class="s2">&quot;kernel-name&quot;</span><span class="p">:</span><span class="s2">&quot;v_multi_scaler:</span><span class="si">{v_multi_scaler_1}</span><span class="s2">&quot;</span><span class="p">,</span>
      <span class="s2">&quot;library-name&quot;</span><span class="p">:</span> <span class="s2">&quot;libvvas_xpreprocessor.so&quot;</span><span class="p">,</span>
      <span class="s2">&quot;config&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;alpha_r&quot;</span> <span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
        <span class="s2">&quot;alpha_g&quot;</span> <span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
        <span class="s2">&quot;alpha_b&quot;</span> <span class="p">:</span> <span class="mi">128</span><span class="p">,</span>
        <span class="s2">&quot;beta_r&quot;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;beta_g&quot;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;beta_b&quot;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
        <span class="s2">&quot;inference-level&quot;</span> <span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="s2">&quot;debug_level&quot;</span> <span class="p">:</span> <span class="mi">0</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">]</span>
<span class="p">}</span>

<span class="n">kernel_pp_plate_num</span><span class="o">.</span><span class="n">json</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
  <span class="s2">&quot;vvas-library-repo&quot;</span><span class="p">:</span> <span class="s2">&quot;/usr/lib/&quot;</span><span class="p">,</span>
  <span class="s2">&quot;inference-level&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
  <span class="s2">&quot;attach-ppe-outbuf&quot;</span><span class="p">:</span> <span class="n">false</span><span class="p">,</span>
  <span class="s2">&quot;kernel&quot;</span> <span class="p">:</span> <span class="p">{</span>
    <span class="s2">&quot;library-name&quot;</span><span class="p">:</span><span class="s2">&quot;libvvas_xdpuinfer.so&quot;</span><span class="p">,</span>
    <span class="s2">&quot;config&quot;</span><span class="p">:</span> <span class="p">{</span>
      <span class="s2">&quot;batch-size&quot;</span> <span class="p">:</span> <span class="mi">0</span><span class="p">,</span>
      <span class="s2">&quot;model-name&quot;</span> <span class="p">:</span> <span class="s2">&quot;plate_num&quot;</span><span class="p">,</span>
      <span class="s2">&quot;model-class&quot;</span> <span class="p">:</span> <span class="s2">&quot;PLATENUM&quot;</span><span class="p">,</span>
      <span class="s2">&quot;model-format&quot;</span> <span class="p">:</span> <span class="s2">&quot;BGR&quot;</span><span class="p">,</span>
      <span class="s2">&quot;model-path&quot;</span> <span class="p">:</span> <span class="s2">&quot;/usr/share/vitis_ai_library/models/&quot;</span><span class="p">,</span>
      <span class="s2">&quot;run_time_model&quot;</span> <span class="p">:</span> <span class="n">false</span><span class="p">,</span>
      <span class="s2">&quot;need_preprocess&quot;</span> <span class="p">:</span> <span class="n">false</span><span class="p">,</span>
      <span class="s2">&quot;performance_test&quot;</span> <span class="p">:</span> <span class="n">false</span><span class="p">,</span>
      <span class="s2">&quot;debug_level&quot;</span> <span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
      <span class="s2">&quot;max-objects&quot;</span><span class="p">:</span><span class="mi">3</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="n">kernel_plate_num</span><span class="o">.</span><span class="n">json</span>
</pre></div>
</div>
<p>Please note the “inference-level” parameter in both the json is 3 which tells the framework that this model placed at level 3 in full use case.</p>
<p>Hope you enjoyed the tutorial.</p>
<p>Now, let’s look into implementing the design and executing using Vitis AI and VVAS.</p>
</section>
</section>
</section>
<section id="example-design-creation">
<h2>Example Design Creation<a class="headerlink" href="#example-design-creation" title="Permalink to this heading">¶</a></h2>
<p>This section covers the steps to create a complete example design that comprise Base Platform, Hardware Accelerators (Kernels) and Software required to run Machine Learning Applications explained in thie Tutorial.</p>
<p>This tutorial needs video codec unit (VCU) decoder, Video Mixer and HDMI Tx, hence select a platform having these IPs.</p>
<p>This tutorial uses the VVAS <a class="reference external" href="https://github.com/Xilinx/VVAS/tree/master/vvas-platforms/Embedded/zcu104_vcuDec_vmixHdmiTx">zcu104_vcuDec_vmixHdmiTx</a> platform because it supports VCU decoder, Video mixer and HDMI Tx subsystem.</p>
<p>For more information on Vitis platforms, see <a class="reference external" href="https://www.xilinx.com/products/design-tools/vitis/vitis-platform.html">Vitis Software Platform</a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>VVAS platform <code class="docutils literal notranslate"><span class="pre">zcu104_vcuDec_vmixHdmiTx</span></code> may not be performance optimal. This platform is made available as reference and for tutorial demonstration.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>VVAS platform <code class="docutils literal notranslate"><span class="pre">zcu104_vcuDec_vmixHdmiTx</span></code> adds patch to irps5401 driver for zcu104 board to support multi thread execution of VAI models.
This <a class="reference external" href="https://github.com/Xilinx/Vitis-AI/blob/v2.0/dsa/DPU-TRD/app/dpu_sw_optimize.tar.gz">patch</a> shouldn’t be applied to other boards
and is not part of the official Xilinx released 2022.1 Petalinux.</p>
</div>
<section id="build-platform">
<h3>Build Platform<a class="headerlink" href="#build-platform" title="Permalink to this heading">¶</a></h3>
<p>The first and foremost step is to build this platform from its sources.</p>
<p>The platform provides the following hardware and software components of the pipeline:</p>
<ul class="simple">
<li><p>VCU hardened IP block</p></li>
<li><p>Video Mixer and HDMI Tx soft IP blocks</p></li>
<li><p>Opensource framework like GStreamer, OpenCV</p></li>
<li><p>Vitis AI 2.5 libraries</p></li>
<li><p>Xilinx Runtime (XRT)</p></li>
<li><p>omxh264dec GStreamer plugin</p></li>
<li><p>kmmsink GStreamer plugin</p></li>
<li><p>VVAS GStreamer plugins and libraries</p>
<ul>
<li><p><a class="reference internal" href="../../common/common_plugins.html#vvas-xinfer"><span class="std std-ref">vvas_xinfer</span></a> GStreamer plugin</p></li>
<li><p><a class="reference internal" href="../../common/common_plugins.html#vvas-xdpuinfer"><span class="std std-ref">vvas_xdpuinfer</span></a> software accelerator library</p></li>
<li><p><a class="reference internal" href="../../common/common_plugins.html#vvas-xboundingbox"><span class="std std-ref">vvas_xboundingbox</span></a> software accelerator library</p></li>
</ul>
</li>
</ul>
<p>Steps for building the platform:</p>
<p>1. Download the VVAS git repository. Let the path where VVAS repo is downloaded be represented as <code class="docutils literal notranslate"><span class="pre">&lt;VVAS_REPO&gt;</span></code>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">git</span> <span class="n">clone</span> <span class="n">https</span><span class="p">:</span><span class="o">//</span><span class="n">github</span><span class="o">.</span><span class="n">com</span><span class="o">/</span><span class="n">Xilinx</span><span class="o">/</span><span class="n">VVAS</span><span class="o">.</span><span class="n">git</span>
</pre></div>
</div>
<p>2. Setup the toolchain</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">source</span> <span class="o">&lt;</span><span class="mf">2022.1</span><span class="n">_Vitis</span><span class="o">&gt;/</span><span class="n">settings64</span><span class="o">.</span><span class="n">sh</span>
<span class="n">source</span> <span class="o">&lt;</span><span class="mf">2022.1</span><span class="n">_Petalinux</span><span class="o">&gt;/</span><span class="n">settings</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
<p>3. Change directory to the platform</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="o">&lt;</span><span class="n">VVAS_REPO</span><span class="o">&gt;/</span><span class="n">VVAS</span><span class="o">/</span><span class="n">vvas</span><span class="o">-</span><span class="n">platforms</span><span class="o">/</span><span class="n">Embedded</span><span class="o">/</span><span class="n">zcu104_vcuDec_vmixHdmiTx</span>
</pre></div>
</div>
<p>4. Build the platform</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">make</span>
</pre></div>
</div>
<p>After the build is finished, the platform is available at
<code class="docutils literal notranslate"><span class="pre">&lt;VVAS_REPO&gt;/VVAS/vvas-platforms/Embedded/zcu104_vcuDec_vmixHdmiTx/platform_repo/xilinx_zcu104_vcuDec_vmixHdmiTx_202210_1/export/xilinx_zcu104_vcuDec_vmixHdmiTx_202210_1/</span></code>.</p>
<p>Let the path to platform be represented as <code class="docutils literal notranslate"><span class="pre">&lt;PLATFORM_PATH&gt;</span></code>.</p>
</section>
<section id="vitis-example-project">
<h3>Vitis Example Project<a class="headerlink" href="#vitis-example-project" title="Permalink to this heading">¶</a></h3>
<p>This section covers the steps to create a final sdcard image from the <strong>platform</strong> created in previous step and hardware accelerators, also called as <strong>kernels</strong>.</p>
<p>A Vitis build is required to stitch all the discussed hardware accelerators to the platform design.
The hardware accelerators required are:</p>
<ol class="arabic simple">
<li><p>DPU (Xilinx ML IP)</p></li>
<li><p>Multiscaler (Xilinx Preprocessing IP)</p></li>
</ol>
<p>The Xilinx deep learning processor unit (DPU) is a configurable computation engine dedicated for convolutional neural networks.
Refer to <a class="reference external" href="https://github.com/Xilinx/Vitis-AI/blob/master/dsa/DPU-TRD/prj/Vitis/README.md">DPU-TRD</a> for more information and compiling the DPU accelerator.</p>
<p>Multiscaler IP/Kernel source code can be refered from &lt;TBD&gt;</p>
<p>The <code class="docutils literal notranslate"><span class="pre">multichannel_ml</span></code> example design adds two instances of B3136 DPU configuration and a single instance of Multiscaler to the <code class="docutils literal notranslate"><span class="pre">zcu104_vcuDec_vmixHdmiTx</span></code> platform.</p>
<p>Steps for building Vitis example project:</p>
<ol class="arabic simple">
<li><p>Download Vitis-AI. Let the path where Vitis-AI is downloaded be represented as <code class="docutils literal notranslate"><span class="pre">&lt;VITIS_AI_REPO&gt;</span></code>.</p></li>
</ol>
<blockquote>
<div><ul class="simple">
<li><p>Open the <a class="reference external" href="https://github.com/Xilinx/Vitis-AI/tree/master/reference_design#readme">reference_design</a> readme page from Vitis-AI release repo.</p></li>
<li><p>Copy the <code class="docutils literal notranslate"><span class="pre">Download</span> <span class="pre">Link</span></code> for <code class="docutils literal notranslate"><span class="pre">IP</span> <span class="pre">Name</span></code> corresponding to <code class="docutils literal notranslate"><span class="pre">DPUCZDX8G</span></code> from <code class="docutils literal notranslate"><span class="pre">Edge</span> <span class="pre">IP</span></code> Table</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">wget</span> <span class="o">-</span><span class="n">O</span> <span class="n">DPUCZDX8G</span><span class="o">.</span><span class="n">tar</span><span class="o">.</span><span class="n">gz</span> <span class="s1">&#39;&lt;Download Link&gt;&#39;</span>
</pre></div>
</div>
<ul class="simple">
<li><p>Uarchive <code class="docutils literal notranslate"><span class="pre">DPUCZDX8G.tar.gz</span></code></p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">tar</span> <span class="o">-</span><span class="n">xf</span> <span class="n">DPUCZDX8G</span><span class="o">.</span><span class="n">tar</span><span class="o">.</span><span class="n">gz</span>
</pre></div>
</div>
</div></blockquote>
<ol class="arabic simple" start="2">
<li><p>Change directory to example project</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="o">&lt;</span><span class="n">VVAS_REPO</span><span class="o">&gt;/</span><span class="n">VVAS</span><span class="o">/</span><span class="n">vvas</span><span class="o">-</span><span class="n">examples</span><span class="o">/</span><span class="n">Embedded</span><span class="o">/</span><span class="n">multichannel_ml</span><span class="o">/</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li><p>Compile the project</p></li>
</ol>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">make</span> <span class="n">PLATFORM</span><span class="o">=&lt;</span><span class="n">PLATFORM_PATH</span><span class="o">&gt;/</span><span class="n">xilinx_zcu104_vcuDec_vmixHdmiTx_202210_1</span><span class="o">.</span><span class="n">xpfm</span> <span class="n">DPU_TRD_PATH</span><span class="o">=&lt;</span><span class="n">VITIS_AI_REPO</span><span class="o">&gt;/</span><span class="n">DPUCZDX8G</span> <span class="n">HW_ACCEL_PATH</span><span class="o">=&lt;</span><span class="n">VVAS_REPO</span><span class="o">&gt;/</span><span class="n">VVAS</span><span class="o">/</span><span class="n">vvas</span><span class="o">-</span><span class="n">accel</span><span class="o">-</span><span class="n">hw</span><span class="o">/</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p><em>Depending on the build machine capacity, building this example project can take about 3 or more hours to compile</em>.</p>
</div>
<p>Once the build is completed, you can find the sdcard image at
<code class="docutils literal notranslate"><span class="pre">&lt;VVAS_REPO&gt;/VVAS/vvas-examples/Embedded/multichannel_ml/binary_container_1/sd_card.img</span></code>.</p>
</section>
<section id="board-bring-up">
<span id="id1"></span><h3>Board bring up<a class="headerlink" href="#board-bring-up" title="Permalink to this heading">¶</a></h3>
<ol class="arabic">
<li><p>Burn the SD card image <code class="docutils literal notranslate"><span class="pre">sd_card.img</span></code> (Either from <a class="reference external" href="https://www.xilinx.com/member/forms/download/xef.html?filename=vvas_multichannel_ml_2022.1_zcu104.zip">Release package</a> or generated)  using a SD card flashing tool like dd, Win32DiskImager, or Balena Etcher.</p>
<p>Boot the board using this SD card.</p>
</li>
<li><p>Once the board is booted, resize the ext4 partition to extend to full SD card size:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">resize</span><span class="o">-</span><span class="n">part</span> <span class="o">/</span><span class="n">dev</span><span class="o">/</span><span class="n">mmcblk0p2</span>
</pre></div>
</div>
</li>
<li><p>From the host system, copy the video files on the board:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mkdir</span> <span class="o">-</span><span class="n">p</span> <span class="o">~/</span><span class="n">videos</span>
<span class="n">scp</span> <span class="o">-</span><span class="n">r</span> <span class="o">&lt;</span><span class="n">Path</span> <span class="n">to</span> <span class="n">Videos</span><span class="o">&gt;</span> <span class="n">root</span><span class="o">@&lt;</span><span class="n">board</span> <span class="n">ip</span><span class="o">&gt;</span><span class="p">:</span><span class="o">~/</span><span class="n">videos</span>
</pre></div>
</div>
</li>
</ol>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Password for <em>root</em> user is <em>root</em>.</p>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Video files are not provided as part of release package.</p>
</div>
<ol class="arabic" start="4">
<li><p>Copy the model json files and scripts on the board:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">scp</span> <span class="o">-</span><span class="n">r</span> <span class="o">&lt;</span><span class="n">RELEASE_PATH</span><span class="o">&gt;/</span><span class="n">vvas_multichannel_ml_2022</span><span class="mf">.1</span><span class="n">_zcu104</span><span class="o">/</span><span class="n">scripts_n_utils</span><span class="o">/</span> <span class="n">root</span><span class="o">@&lt;</span><span class="n">board</span> <span class="n">ip</span><span class="o">&gt;</span><span class="p">:</span><span class="o">~</span>
</pre></div>
</div>
</li>
<li><p>Copy the Vitis-AI model files on board. Execute the command mentioned below on the target board:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">mkdir</span> <span class="o">-</span><span class="n">p</span> <span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="n">share</span><span class="o">/</span><span class="n">vitis_ai_library</span><span class="o">/</span><span class="n">models</span>
<span class="n">scp</span> <span class="o">-</span><span class="n">r</span> <span class="o">&lt;</span><span class="n">RELEASE_PATH</span><span class="o">&gt;/</span><span class="n">vvas_multichannel_ml_2022</span><span class="mf">.1</span><span class="n">_zcu104</span><span class="o">/</span><span class="n">models</span><span class="o">/*</span> <span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="n">share</span><span class="o">/</span><span class="n">vitis_ai_library</span><span class="o">/</span><span class="n">models</span><span class="o">/</span>
</pre></div>
</div>
</li>
<li><p>Execute four channel GStreamer pipeline script. Execute the command mentioned below on the target board:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="o">~/</span><span class="n">scripts_n_utils</span><span class="o">/</span><span class="n">multichannel_ml</span><span class="o">/</span>
<span class="o">./</span><span class="n">multichannel_ml</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
</li>
</ol>
<p>You can now see the 4-channel mixed video on the HDMI monitor.</p>
<ol class="arabic" start="7">
<li><p>Execute multi level cascade Gstreamer pipeline scripts:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">cd</span> <span class="o">~/</span><span class="n">scripts_n_utils</span><span class="o">/</span><span class="n">cascade</span><span class="o">/</span>
<span class="o">./</span><span class="mi">1</span><span class="n">_level_cascade</span><span class="o">.</span><span class="n">sh</span>
<span class="o">./</span><span class="mi">2</span><span class="n">_level_cascade</span><span class="o">.</span><span class="n">sh</span>
<span class="o">./</span><span class="mi">3</span><span class="n">_level_cascade</span><span class="o">.</span><span class="n">sh</span>
</pre></div>
</div>
</li>
</ol>
</section>
</section>
<section id="known-issues">
<h2>Known Issues<a class="headerlink" href="#known-issues" title="Permalink to this heading">¶</a></h2>
<ul class="simple">
<li><p>On zcu104 boards, Cascaded pipelines OR several ML instances running simultaneously are sending board into bad state and needs reboot to recover from it. The default value of IOUT_OC_FAULT_LIMIT on PMIC chip irps5401 is too low  and that is causing the temperature fault limit getting crossed. Workaround is to increase this limit. But there is risk of board getting damaged if running for long time.</p></li>
</ul>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">¶</a></h2>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://github.com/Xilinx/Vitis-AI">https://github.com/Xilinx/Vitis-AI</a></p></li>
<li><p><a class="reference external" href="https://www.xilinx.com/html_docs/vitis_ai/2_0/index.html">https://www.xilinx.com/html_docs/vitis_ai/2_0/index.html</a></p></li>
<li><p><a class="reference external" href="https://www.xilinx.com/support/download/index.html/content/xilinx/en/downloadNav/embedded-designtools.html">https://www.xilinx.com/support/download/index.html/content/xilinx/en/downloadNav/embedded-designtools.html</a></p></li>
<li><p><a class="reference external" href="https://www.xilinx.com/products/boards-and-kits/zcu104.html">https://www.xilinx.com/products/boards-and-kits/zcu104.html</a></p></li>
<li><p><a class="reference external" href="https://www.xilinx.com/support/documentation/ip_documentation/vcu/v1_2/pg252-vcu.pdf">https://www.xilinx.com/support/documentation/ip_documentation/vcu/v1_2/pg252-vcu.pdf</a></p></li>
<li><p><a class="reference external" href="https://gstreamer.freedesktop.org">https://gstreamer.freedesktop.org</a></p></li>
<li><p><a class="reference external" href="https://www.kernel.org/doc/html/v4.13/gpu/drm-kms.html">https://www.kernel.org/doc/html/v4.13/gpu/drm-kms.html</a></p></li>
<li><p><a class="reference external" href="https://gstreamer.freedesktop.org/documentation/kms/index.html">https://gstreamer.freedesktop.org/documentation/kms/index.html</a></p></li>
</ol>
</section>
</section>


           </div>
           
          </div>
          <footer>
<!-- Atalwar: Moved the footer code to layout.html to resolve conflict with the Xilinx template -->
</footer>

        </div>
      </div>


	  <!-- Sphinx Page Footer block -->
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../embedded-plugins.html" class="btn btn-neutral float-right" title="VVAS GStreamer Plug-ins for Embedded Platforms" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../smart_model_select.html" class="btn btn-neutral float-left" title="Smart Model Select Application" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo" class="copyright">
    <p class="footerinfo">
      <span class="lastupdated">
        Last updated on June 22, 2022.
      </span>

    </p>
	<br>
  </div>
      </div>
    </section>


  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

   <script type="text/javascript">
    jQuery(function() { Search.loadIndex("searchindex.js"); });
  </script>

  <script type="text/javascript" id="searchindexloader"></script>


  
  
    
  



  <!--  Xilinx template footer block -->
							</div>
						</div>
					</div>
				</div>
				<div class="xilinxExperienceFragments experiencefragment aem-GridColumn--default--none aem-GridColumn aem-GridColumn--offset--default--0 aem-GridColumn--default--16">
					<div class="xf-content-height">
						<div class="aem-Grid aem-Grid--16 aem-Grid--default--16 ">
							<div class="footer parbase aem-GridColumn--default--none aem-GridColumn aem-GridColumn--offset--default--0 aem-GridColumn--default--16">
								<noindex>
                  <!-- make footer fixed - NileshP -->
                  <style>
                        .footer {
                        position: fixed;
                        left: 0;
                        bottom: 0;
                        width: 100%;
                        }
                  </style>
                  <!-- make footer fixed NileshP-->
									<footer>
										<div class="aem-Grid aem-Grid--16">
											<div class="aem-GridColumn aem-GridColumn--xxxlarge--none aem-GridColumn--xsmall--16 aem-GridColumn--offset--xsmall--0 aem-GridColumn--xlarge--none aem-GridColumn--xxlarge--none aem-GridColumn--default--none aem-GridColumn--offset--large--1 aem-GridColumn--xlarge--12 aem-GridColumn--offset--default--0 aem-GridColumn--xxlarge--10 aem-GridColumn--offset--xlarge--2 aem-GridColumn--offset--xxlarge--3 aem-GridColumn--offset--xxxlarge--4 aem-GridColumn--xsmall--none aem-GridColumn--large--none aem-GridColumn aem-GridColumn--large--14 aem-GridColumn--xxxlarge--8 aem-GridColumn--default--16">
												<div class="container-fluid sub-footer">
													<div class="row">
														<div class="footerSocial parbase">
														
															<div class="col-md-push-6 col-lg-push-6 col-md-6 col-lg-6">
																<div class="lang-select dropup hidden-md hidden-lg">
		<button data-toggle="dropdown">
			<span class="fas fa-globe" aria-hidden="true"></span>
			<span>
				
					English
				</span>
		<span class="far fa-angle-down" aria-hidden="true"></span>
	</button>
	<ul class="dropdown-menu">
		<li>
				<a target="_blank" href="https://japan.xilinx.com/" target="_self">
					日本語
				</a>
			</li>
		<li>
				<a target="_blank" href="https://china.xilinx.com/" target="_self">
					简体中文
				</a>
			</li>
		</ul>
	</div>
																<ul class="list-inline pull-right social-menu">
																	<li>
																		<a target="_blank" href="https://www.linkedin.com/company/xilinx">
																		<span class="linkedin icon"></span>
																		<span class="sr-only">Connect on LinkedIn</span>
																		</a>
																	</li>
																	<li>
																		<a target="_blank" href="https://www.twitter.com/XilinxInc">
																		<span class="twitter icon"></span>
																		<span class="sr-only">Follow us on Twitter</span>
																		</a>
																	</li>
																	<li>
																		<a target="_blank" href="https://www.facebook.com/XilinxInc">
																		<span class="facebook icon"></span>
																		<span class="sr-only">Connect on Facebook</span>
																		</a>
																	</li>
																	<li>
																		<a target="_blank" href="https://www.youtube.com/XilinxInc">
																		<span class="youtube icon"></span>
																		<span class="sr-only">Watch us on YouTube</span>
																		</a>
																	</li>
																	<li>
																		<a target="_blank" href="https://www.xilinx.com/registration/subscriber-signup.html">
																		<span class="newsletter icon"></span>
																		<span class="sr-only">Subscribe to Newsletter</span>
																		</a>
																	</li>
																</ul>
																	<div class="lang-select dropup hidden-xs hidden-sm">
	<button data-toggle="dropdown">
		<span class="fas fa-globe" aria-hidden="true"></span>
		<span>
			
				English
			</span>
		<span class="far fa-angle-down" aria-hidden="true"></span>
	</button>
	<ul class="dropdown-menu">
		<li>
				<a target="_blank" href="https://japan.xilinx.com/" target="_self">
					日本語
				</a>
			</li>
		<li>
				<a target="_blank" href="https://china.xilinx.com/" target="_self">
					简体中文
				</a>
			</li>
		</ul>
	</div>
															</div>
														</div>
														<div class="col-md-pull-5 col-lg-pull-5 col-md-5 col-lg-5">
															<span class="copyright">©2022 Advanced Micro Devices, Inc</span>
														</div>

													</div>
													                    <div class="movethisrowtoleft row">
                        <div class="col-xs-24">
                            <ul class="sub-menu">
                                <li><a target="_blank" href="https://www.amd.com/en/corporate/copyright">Terms and Conditions</a></li>
                                <li><a target="_blank" href="https://www.amd.com/en/corporate/privacy">Privacy</a></li>
                                <li><a target="_blank" href="https://www.amd.com/en/corporate/cookies">Cookie Policy</a></li>
                                <li><a target="_blank" href="https://www.amd.com/en/corporate/trademarks">Trademarks</a></li>
                                <li><a target="_blank" href="https://www.amd.com/system/files/documents/statement-human-trafficking-forced-labor.pdf">Statement on Forced Labor</a></li>
                                <li><a target="_blank" href="https://www.amd.com/en/corporate/competition">Fair and Open Competition</a></li>
                                <li><a target="_blank" href="https://www.amd.com/system/files/documents/amd-uk-tax-strategy.pdf">UK Tax Strategy</a></li>
								<li><a target="_blank" href="https://docs.xilinx.com/v/u/9x6YvZKuWyhJId7y7RQQKA">Inclusive Terminology</a></li>
                                <li><a href="#cookiessettings" class="ot-sdk-show-settings">Cookies Settings</a></li>
                            </ul>
                        </div>
                    </div>
												</div>
											</div>
										</div>
									</footer>
								</noindex>
							</div>
						</div>
					</div>
				</div>
<div class="backToTop parbase aem-GridColumn--default--none aem-GridColumn aem-GridColumn--offset--default--0 aem-GridColumn--default--16"><noindex>
    <span data-component="backToTopButton" class="backToTopButton loaded">
        <ul>
            <li>
                <a href="https://www.author.xilinx.com/xx/rebrand/amd/en-amd-xilinx-header-footer.html#top" class="btn top">
                    <span class="fas fa-angle-up" aria-hidden="true"></span>
                </a>
            </li>
        </ul>
    </span>
</noindex></div>
			</div>
		</div>


		<script>window.CQ = window.CQ || {}</script>
		<script src="https://static.cloud.coveo.com/searchui/v2.4382/js/CoveoJsSearch.Lazy.min.js"></script>
		<script>
			var underscoreSetup = function () {
			  _.templateSettings.interpolate = /\{\{=([^-][\S\s]+?)\}\}/g;
			  _.templateSettings.evaluate = /\{\{([^-=][\S\s]+?)\}\}/g;
			  _.templateSettings.escape = /\{\{-([^=][\S\s]+?)\}\}/g;
			}

			underscoreSetup();
		</script>
	</body>
</html>